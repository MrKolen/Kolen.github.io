<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>基于无监督的事件抽取 | Kolen's Nest</title><meta name="description" content="一篇非正式的综述，对12篇论文的总结"><meta name="keywords" content="自然语言处理,事件抽取,信息抽取,知识图谱"><meta name="author" content="Kolen"><meta name="copyright" content="Kolen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="基于无监督的事件抽取"><meta name="twitter:description" content="一篇非正式的综述，对12篇论文的总结"><meta name="twitter:image" content="https://pic2.zhimg.com/v2-53b29187538200f75ad18f8784deb024_1200x500.jpg"><meta property="og:type" content="article"><meta property="og:title" content="基于无监督的事件抽取"><meta property="og:url" content="http://mrkolen.github.io/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/"><meta property="og:site_name" content="Kolen's Nest"><meta property="og:description" content="一篇非正式的综述，对12篇论文的总结"><meta property="og:image" content="https://pic2.zhimg.com/v2-53b29187538200f75ad18f8784deb024_1200x500.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="http://mrkolen.github.io/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/"><link rel="next" title="Transformers其实是图神经网络" href="http://mrkolen.github.io/2020/03/03/Transformers%E5%85%B6%E5%AE%9E%E6%98%AF%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://mrkolen.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"languages":{"author":"作者: Kolen","link":"链接: http://mrkolen.github.io/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/","source":"来源: Kolen's Nest","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Kolen's Nest" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Kolen's Nest</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">9</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#理论基础"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> 理论基础</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#技术方法和研究现状"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> 技术方法和研究现状</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#聚类"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> 聚类</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#基于特征选择"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text"> 基于特征选择</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#基于概率模型"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text"> 基于概率模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#基于信息检索"><span class="toc_mobile_items-number">2.1.3.</span> <span class="toc_mobile_items-text"> 基于信息检索</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#基于联合模型"><span class="toc_mobile_items-number">2.1.4.</span> <span class="toc_mobile_items-text"> 基于联合模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#基于图结构"><span class="toc_mobile_items-number">2.1.5.</span> <span class="toc_mobile_items-text"> 基于图结构</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#对抗式生成网络gan"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text"> 对抗式生成网络（GAN）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#隐狄利克雷分布lda"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text"> 隐狄利克雷分布（LDA）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#深度信念网络dbn"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text"> 深度信念网络（DBN）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#异常检测"><span class="toc_mobile_items-number">2.5.</span> <span class="toc_mobile_items-text"> 异常检测</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#总结"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> 总结</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#参考文献"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> 参考文献</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#理论基础"><span class="toc-number">1.</span> <span class="toc-text"> 理论基础</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#技术方法和研究现状"><span class="toc-number">2.</span> <span class="toc-text"> 技术方法和研究现状</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#聚类"><span class="toc-number">2.1.</span> <span class="toc-text"> 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于特征选择"><span class="toc-number">2.1.1.</span> <span class="toc-text"> 基于特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于概率模型"><span class="toc-number">2.1.2.</span> <span class="toc-text"> 基于概率模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于信息检索"><span class="toc-number">2.1.3.</span> <span class="toc-text"> 基于信息检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于联合模型"><span class="toc-number">2.1.4.</span> <span class="toc-text"> 基于联合模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于图结构"><span class="toc-number">2.1.5.</span> <span class="toc-text"> 基于图结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#对抗式生成网络gan"><span class="toc-number">2.2.</span> <span class="toc-text"> 对抗式生成网络（GAN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#隐狄利克雷分布lda"><span class="toc-number">2.3.</span> <span class="toc-text"> 隐狄利克雷分布（LDA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度信念网络dbn"><span class="toc-number">2.4.</span> <span class="toc-text"> 深度信念网络（DBN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#异常检测"><span class="toc-number">2.5.</span> <span class="toc-text"> 异常检测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-number">3.</span> <span class="toc-text"> 总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考文献"><span class="toc-number">4.</span> <span class="toc-text"> 参考文献</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://pic2.zhimg.com/v2-53b29187538200f75ad18f8784deb024_1200x500.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">基于无监督的事件抽取</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-04-24<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-04-24</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.5k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 8 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="理论基础"><a class="markdownIt-Anchor" href="#理论基础"></a> 理论基础</h1>
<p>目前无监督事件抽取方法主要有两种理论基础：</p>
<ol>
<li><strong>分布假设（Distributional Hypothesis）：</strong> 如果两个词的用法相似及出现在相同上下文中，那么这两个词就意思相近。相应的，在事件抽取中，如果候选事件触发词或者候选事件元素具有相似的语境，那么这些候选事件触发词倾向于触发相同类型的事件，相应的候选事件元素倾向于扮演相同的事件元素；</li>
<li><strong>强化学习（Reinforcement Learning, RL）：</strong> 假设实例和标签会产生不同程度的困难，并且预期收益和惩罚（Rewards）是不同的，根据实际真相（Expert）和抽取器（Agent）所做的标记之间的差异利用鉴别器来估计适当的报酬，这样模型就会自动学习到如何正确识别出事件。</li>
</ol>
<h1 id="技术方法和研究现状"><a class="markdownIt-Anchor" href="#技术方法和研究现状"></a> 技术方法和研究现状</h1>
<h2 id="聚类"><a class="markdownIt-Anchor" href="#聚类"></a> 聚类</h2>
<h3 id="基于特征选择"><a class="markdownIt-Anchor" href="#基于特征选择"></a> 基于特征选择</h3>
<p>Nallapati[1] 提出使用余弦相似度、地点和人物特征的加权和来计算新闻间的语义相似性，然后将时间跨度纳入新闻相似度衰减的考量中，最后利用层次聚类进行事件的抽取。<br>
Jia 等[2] 借鉴Single-Pass 聚类算法思想进行新闻事件探测，图2-1为其算法流程图，该方法首先统计分析文档中的词汇，然后根据词频筛选出文档特征，再由文档特征得到事件模板（即事件的关键词集），之后借鉴倒排文档频率（IDF）思想过滤事件关键词，从而更新事件模板，最后使用聚类的方式从事件模板中抽取事件，聚类过程中考虑了时间间隔因素来提高分类精度。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/2.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="Jia提出的事件主要元素的学习过程" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/2.png"></a>
<p>Stokes 等[3] 提出了一种基于文档多层表示的事件抽取方法，不仅利用WordNet 来构建文本词汇链从而对文档进行深层语义表示，还利用专有名词构建文档的句法表示，最后同时利用两种表示用聚类的方式对事件进行抽取。</p>
<h3 id="基于概率模型"><a class="markdownIt-Anchor" href="#基于概率模型"></a> 基于概率模型</h3>
<p>Li 等[4] 利用概率模型完成了新事件的识别和抽取，首先将事件（Event）定义为由人物、地点、关键字和时间构成的集合，然后分别使用单元语法模型（Unigram Model）和高斯模型对事件内容的三个特征（Person、Location、Keywords）和事件时间单独建模，整体模型框架如图 2-2所示。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/4a.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="新闻文章生成模型的图形模型表示" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/4a.png"></a>
<p>之后通过hill-climbing算法找到文章数量随时间变化的分布函数的峰值点，即对应待抽取出的事件类别数，如图 2-3所示。最后使用最大似然估计来构建目标函数，并使用EM算法对上述参数进行优化迭代。但是该算法选取某类事件中的一篇文章来表示抽取出的事件。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/4b.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="hill-climbing算法找到分布的峰值" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/4b.png"></a>
<h3 id="基于信息检索"><a class="markdownIt-Anchor" href="#基于信息检索"></a> 基于信息检索</h3>
<p>Yang 等[5] 提出将信息检索的方法应用到基于事件抽取的问答任务中，如图 2-4所示，首先使用Web搜索引擎检索出更多相关的事件元素，然后利用WordNet中的语义信息进行筛选，最后使用线性加权法和构建事件语义框架法分别得到两组扩展的元素集。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/5.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="通过信息检索抽取事件" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/5.png"></a>
<h3 id="基于联合模型"><a class="markdownIt-Anchor" href="#基于联合模型"></a> 基于联合模型</h3>
<p>值得关注的是Huang 等[6] 提出联合抽取事件和事件结构信息的方法，如图 2-5所示，首先该方法利用符号特征构建事件语义框架，同时利用分布式特征来表示候选事件触发词和事件元素的词汇级特征，然后使用自动递归编码，将有上述两者联合建模成基于张量的事件结构树，之后利用一个聚类模型同时抽取事件的触发词、事件元素和事件元素扮演的角色。最后，为了确定每个类别的含义，使用了FrameNet、PropBank等外部知识对类别命名。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/6.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="Huang提出的事件抽取框架" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/6.png"></a>
<h3 id="基于图结构"><a class="markdownIt-Anchor" href="#基于图结构"></a> 基于图结构</h3>
<p>比较有意思的是Kuzey 等[7] 将每个新闻文档看成一个节点，并通过新闻之间的相似度建立节点之间的边，形成图的结构，基于图对新闻文档进行聚类，每个类作为一个事件，同时得到事件之间的时序和层次关系（如图2-6所示）。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/7.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="基于图结构的事件聚类示例" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/7.png"></a>
<h2 id="对抗式生成网络gan"><a class="markdownIt-Anchor" href="#对抗式生成网络gan"></a> 对抗式生成网络（GAN）</h2>
<p>最近两年出现了使用GANs进行事件抽取建模，在PipeLine和Joint两种模式下都展露出很好的效果。<br>
Zhang等[8] 用强化学习（RL）的方法对事件抽取任务建模，将事件抽取器看作RL中的Agent，并使用GAN作为RL框架中的Reward机制从而影响抽取器的决策（如图 2-7所示）。之后Zhang[9] 还将这类模型运用到联合事件抽取任务上，也取得不错的效果。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/8.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="使用GAN作为RL中的Reward Function" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/8.png"></a>
<p>Wang等[10] 将GAN引入开放域事件抽取，使得模型效果获得了显著提升。该模型架构如图 2 8所示，首先从语料中采样从而获得文档表示，然后使用生成网络产生含有实体、地点、关键词和时间特征分布的假文档，接着将两类文档整合并使用鉴别网络从所有文档中鉴别出真文档，从而完成抽取。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/10.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="在开放域事件抽取中使用GAN" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/10.png"></a>
<h2 id="隐狄利克雷分布lda"><a class="markdownIt-Anchor" href="#隐狄利克雷分布lda"></a> 隐狄利克雷分布（LDA）</h2>
<p>NewsMiner[11] 通过LDA模型将新闻按照事件组织，并分析新闻和评论之间的联系，在对事件、话题以及实体之间的关系深入分析的基础上提供新闻多刻面搜索。随着相似事件的不断重复发生，事件知识可以通过增量学习得到积累完善。</p>
<h2 id="深度信念网络dbn"><a class="markdownIt-Anchor" href="#深度信念网络dbn"></a> 深度信念网络（DBN）</h2>
<p>Zhang等[12] 提出一种基于深度信念网络的事件识别模型。首先通过分词系统从CEC2.0中文语料中获得候选词并将它们分为五种类型（触发词、参与者、对象、时间和地点），然后选择六种识别特征（词性、依存语法、长度、位置、词与核心词的距离和触发词频度）并制定相应的特征表示规则来生成候选词的特征向量，最后通过深度信念网络抽取词的深层语义信息，并由BP神经网络识别事件。同时还提出了一种融合无监督和有监督两种学习方式的混合监督深度信念网络，提高了识别精度，并缩短了训练时间。图2-9为两种网络性能对比。</p>
<a href="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/12.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img class="lazyload" title="两种分类器识别效果稳定性的对比图" data-src="/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/12.png"></a>
<h2 id="异常检测"><a class="markdownIt-Anchor" href="#异常检测"></a> 异常检测</h2>
<p>该方法首先假设某个重大事件的发生会导致新闻媒体或社交网络上涌现出大量的相关报道或讨论；反之，关于某一主题的报道或讨论突然增多则暗示着某一重大事件的发生。一般是用统计的方法对文档整体的异常情况进行分析或对每个词频进行异常检测，该方法只能识别出新事件，不能识别其具体信息。</p>
<h1 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h1>
<ol>
<li>无监督的方法在事件抽取方面，研究起步较晚，且发展相对缓慢。较多的学者采用相似度聚类的方法进行事件识别和抽取，工作大多聚焦在数据预处理、特征选取或聚类算法改进环节上；</li>
<li>无监督事件抽取方法可以发现新的事件，但其发现的新事件往往是相似模板的聚类，难以规则化，很难被用来构建知识库，需要将其同现有知识库的事件框架进行对齐，或者通过人工的方式来给每个聚类事件簇赋予语义信息；</li>
<li>事件抽取可以采用数据挖掘中的方法，类似的课题有新闻热点跟踪、主题探测和追踪(topic detection and t racking , TDT)等研究课题。</li>
</ol>
<h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1>
<p>[1]	Ramesh Nallapati, Ao Feng, Fuchun Peng, and James Allan. Event threading within news topics. In Proceedings of the thirteenth Association for Computing Machinery international conference on Information and knowledge management, pages 446–453. Association for Computing Machinery, 2004.<br>
[2]	Jia Ziyan, He Qing, Zhang Haijun, Li Jiayou, and Shi Zhongzhi. A news event detection and tracking algorithm based on dynamic evolution model. Journal of Computer Research and Development, 41(7):1273–1280, 2004.<br>
[3]	Nicola Stokes and Joe Carthy. Combining semantic and syntactic document classifiers to improve first story detection. In Proceedings of the 24th annual international Association for Computing Machinery SIGIR conference on Research and development in information retrieval, pages 424–425. Association for Computing Machinery, 2001.<br>
[4]	Zhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma. A probabilistic model for retrospective news event detection. In Proceedings of the 28th annual international Association for Computing Machinery SIGIR conference on Research and development in information retrieval, pages 106–113. Association for Computing Machinery, 2005.<br>
[5]	Hui Yang, Tat-Seng Chua, Shuguang Wang, and Chun-Keat Koh. Structured use of external knowledge for event-based open domain question answering. In Proceedings of the 26th annual international Association for Computing Machinery SIGIR conference on Research and development in informaion retrieval, pages 33–40. Association for Computing Machinery, 2003.<br>
[6]	Lifu Huang, Xiaocheng Feng, Heng Ji, and Jiawei Han. Liberal event extraction and event schema induction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 2016.<br>
[7]	E. Kuzey, J. Vreeken, G. Weikum, a fresh look on knowledge bases: Distilling named events from news, In Proceedings of the 23rd ACM International Conference on Information and Knowledge Management, 2014, pp. 1689–1698.<br>
[8]	Tongtao Zhang and Heng Ji. 2018. Event extraction with generative adversarial imitation learning. arXiv preprint arXiv:1804.07881.<br>
[9]	Tongtao Zhang, Heng Ji, and Avirup Sil. Joint Entity and Event Extraction with Generative Adversarial Imitation Learning. Data Intelligence 2019 1:2, 99-120.<br>
[10]	Wang, R., Zhou, D., and He, Y. Open event extraction from online text using a generative adversarial network. arXiv preprint arXiv:1908.09246, 2019.<br>
[11]	Hou L, Li J, Wang Z, et al. Newsminer: multifaceted news analysis for event search[J]. Knowledge-Based Systems, 2015, 76: 17-29.<br>
[12]	张亚军,刘宗田,周文. 基于深度信念网络的事件识别[J]. 电子学报,2017,45(06):1415-1423.</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kolen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://mrkolen.github.io/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/">http://mrkolen.github.io/2020/04/24/%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://MrKolen.github.io">Kolen's Nest</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理    </a><a class="post-meta__tags" href="/tags/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96/">事件抽取    </a><a class="post-meta__tags" href="/tags/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/">信息抽取    </a><a class="post-meta__tags" href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱    </a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-53b29187538200f75ad18f8784deb024_1200x500.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/03/03/Transformers%E5%85%B6%E5%AE%9E%E6%98%AF%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><img class="next_cover lazyload" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1583221477231&amp;di=9b99a562e91ac8e3dcd454bb9e30270c&amp;imgtype=0&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180621%2F40ed7a17fe6c42b6a0f914013c9adc9f.jpeg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Transformers其实是图神经网络</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/03/Transformers其实是图神经网络/" title="Transformers其实是图神经网络"><img class="relatedPosts_cover lazyload"data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1583221477231&di=9b99a562e91ac8e3dcd454bb9e30270c&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180621%2F40ed7a17fe6c42b6a0f914013c9adc9f.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-03</div><div class="relatedPosts_title">Transformers其实是图神经网络</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/23/深度学习05-Word2Vec/" title="深度学习05-Word2Vec"><img class="relatedPosts_cover lazyload"data-src="http://n1.cmsfile.pg0.cn/group2/M00/32/68/Cgqg2Vbk5X-Abvn2AABwJC-MKQY090.jpg?enable=&w=550&h=263&cut="><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-23</div><div class="relatedPosts_title">深度学习05-Word2Vec</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/23/深度学习04-进化吧，RNN/" title="深度学习04-进化吧，RNN"><img class="relatedPosts_cover lazyload"data-src="http://n1.cmsfile.pg0.cn/group2/M00/32/68/Cgqg2Vbk5X-Abvn2AABwJC-MKQY090.jpg?enable=&w=550&h=263&cut="><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-23</div><div class="relatedPosts_title">深度学习04-进化吧，RNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/16/深度学习03-循环神经网络/" title="深度学习03-循环神经网络"><img class="relatedPosts_cover lazyload"data-src="http://n1.cmsfile.pg0.cn/group2/M00/32/68/Cgqg2Vbk5X-Abvn2AABwJC-MKQY090.jpg?enable=&w=550&h=263&cut="><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-16</div><div class="relatedPosts_title">深度学习03-循环神经网络</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/16/深度学习02-N-gram语言模型/" title="深度学习02-N-gram语言模型"><img class="relatedPosts_cover lazyload"data-src="http://n1.cmsfile.pg0.cn/group2/M00/32/68/Cgqg2Vbk5X-Abvn2AABwJC-MKQY090.jpg?enable=&w=550&h=263&cut="><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-16</div><div class="relatedPosts_title">深度学习02-N-gram语言模型</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/14/深度学习01-文本预处理/" title="深度学习01-文本预处理"><img class="relatedPosts_cover lazyload"data-src="http://n1.cmsfile.pg0.cn/group2/M00/32/68/Cgqg2Vbk5X-Abvn2AABwJC-MKQY090.jpg?enable=&w=550&h=263&cut="><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-14</div><div class="relatedPosts_title">深度学习01-文本预处理</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'b0Sq5vgXDcUlyeHrLpdWg49H-gzGzoHsz',
  appKey:'jCc17LyuLo4JVbL8Nz2uXqHI',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(https://pic2.zhimg.com/v2-53b29187538200f75ad18f8784deb024_1200x500.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Kolen</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>