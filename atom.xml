<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kolen&#39;s Nest</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://MrKolen.github.io/"/>
  <updated>2020-02-15T12:17:13.813Z</updated>
  <id>http://MrKolen.github.io/</id>
  
  <author>
    <name>Kolen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习01-文本预处理</title>
    <link href="http://MrKolen.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A001-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://MrKolen.github.io/2020/02/14/深度学习01-文本预处理/</id>
    <published>2020-02-14T13:29:36.000Z</published>
    <updated>2020-02-15T12:17:13.813Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h1><hr><p>文本预处理通常包括四个步骤：</p><ol><li>读入文本；</li><li>分词；</li><li>建立字典，将每个词映射到一个唯一的索引；</li><li>将文本从词的序列转换为索引的序列，方便输入模型。</li></ol><h1 id="读入文本"><a class="markdownIt-Anchor" href="#读入文本"></a> 读入文本</h1><hr><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> to:11</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div><p><code>strip()</code>： 去掉前/后缀的空白字符，也就是空格、制表、换行符；<br><code>lower()</code>: 把所有大写字符转换为小写；<br><code>re.sub（‘正则表达式’，‘ ’，String）</code>: 正则表达式的替换函数。将String中将与正则表达式相匹配的字符串转换为空格；<br><code>'[^a-z]+'</code>: <code>^</code>表示“非”，<code>^a-z</code>即为非小写字符，<code>+</code>表示字符串长度至少为1，所以整个表达的意思是所有的非小写字符构成非空字符串。</p><h1 id="分词"><a class="markdownIt-Anchor" href="#分词"></a> 分词</h1><hr><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> title_test from:13 to:23</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div><p><code>@param sentence</code>: 一个列表，每个列表元素都是一个句子；<br><code>@param token</code>: 选定分词级别。<code>word</code>为单词级别，将<code>sentence</code>中的每个句子按空格分割；<code>char</code>为字符级别，将<code>sentence</code>中每个字符串句子转化为列表。</p><h1 id="建立字典"><a class="markdownIt-Anchor" href="#建立字典"></a> 建立字典</h1><hr><p>为了方便模型处理，需要将字符串转化为数字，因此需要先构建一个字典，将每个词映射到一个唯一的索引编号。</p><p>构建的基本过程：</p><ol><li>统计词频</li><li>将语料库中的词去重；</li><li>筛选掉一些；</li><li>添加自己定义的token；</li><li>将每一个token映射到唯一的索引，同时把每一个索引再映射到对应的token。</li></ol><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> from:25 to:66</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div><p><code>__init__</code>: 构造函数。其中<code>tokens</code>为语料库中所有的词，即‘分词’中返回的列表；<code>min_freq</code>为最小词频筛选阈值，词频小于这个数则会被剔除；<code>use_spcial_tokens</code>为标志位。</p><p><code>pad</code>token:同一个batch（比如是个二维的矩阵，矩阵的每一行代表一个句子）的每个句子的长度可能不一样，所以在句子后补上若干个<code>pad</code>，使得和最长的句子长度一样</p><h1 id="使用现有分词工具"><a class="markdownIt-Anchor" href="#使用现有分词工具"></a> 使用现有分词工具</h1><hr><p>上述方法有以下几个缺点:</p><ol><li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li><li>类似“shouldn’t&quot;, &quot;doesn’t&quot;这样的词会被错误地处理</li><li>类似&quot;Mr.&quot;, &quot;Dr.&quot;这样的词会被错误地处理</li></ol><p>在这里简单介绍两个分词工具：spaCy和NLTK。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> from:68 to:68</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div><h2 id="spacy"><a class="markdownIt-Anchor" href="#spacy"></a> spaCy</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> from:70 to:73</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div><h2 id="nltk"><a class="markdownIt-Anchor" href="#nltk"></a> NLTK</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><figcaption><span> from:75 to:78</span><a href="/downloads/code/deeplearning_01.py">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab = Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#简介&quot;&gt;&lt;/a&gt; 简介&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;文本预处理通常包括四个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读入文本；&lt;/li&gt;
&lt;li&gt;分词；&lt;/li&gt;
&lt;li&gt;建立字典，将每个词映射
      
    
    </summary>
    
      <category term="工程项目练习" scheme="http://MrKolen.github.io/categories/%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://MrKolen.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="文本信息挖掘" scheme="http://MrKolen.github.io/tags/%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98/"/>
    
      <category term="信息抽取" scheme="http://MrKolen.github.io/tags/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/"/>
    
      <category term="Pytorch" scheme="http://MrKolen.github.io/tags/Pytorch/"/>
    
      <category term="Python" scheme="http://MrKolen.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>分享两首歌，无摘要</title>
    <link href="http://MrKolen.github.io/2019/05/08/%E5%88%86%E4%BA%AB%E4%B8%A4%E9%A6%96%E6%AD%8C%EF%BC%8C%E6%97%A0%E6%91%98%E8%A6%81/"/>
    <id>http://MrKolen.github.io/2019/05/08/分享两首歌，无摘要/</id>
    <published>2019-05-08T13:50:20.000Z</published>
    <updated>2020-02-14T18:15:13.480Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="慢慢喜欢你"><a class="markdownIt-Anchor" href="#慢慢喜欢你"></a> 慢慢喜欢你</h1><hr><p><a href="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1557335208104&amp;di=f68b4d6f9ea2cf6d8f5fdcb07144742d&amp;imgtype=0&amp;src=http%3A%2F%2Fi1.hdslb.com%2Fbfs%2Farchive%2Fe1ebadef968d0da8a8d78fde059957040de67f4e.jpg" data-fancybox="group" data-caption class="fancybox"><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1557335208104&amp;di=f68b4d6f9ea2cf6d8f5fdcb07144742d&amp;imgtype=0&amp;src=http%3A%2F%2Fi1.hdslb.com%2Fbfs%2Farchive%2Fe1ebadef968d0da8a8d78fde059957040de67f4e.jpg" alt title></a></p><p>书里总爱写到喜出望外的傍晚</p><p>骑的单车还有他和她的对谈</p><p>女孩的白色衣裳男孩爱看她穿</p><p>好多桥段</p><p>好多都浪漫</p><p>好多人心酸</p><p>好聚好散</p><p>好多天都看不完</p><p>刚才吻了你一下你也喜欢对吗</p><p>不然怎么一直牵我的手不放</p><p>你说你好想带我回去你的家乡</p><p>绿瓦红砖</p><p>柳树和青苔</p><p>过去和现在</p><p>你说你也会这样</p><p>慢慢喜欢你</p><p>慢慢的亲密</p><p>慢慢聊自己</p><p>慢慢和你走在一起</p><p>慢慢我想配合你</p><p>慢慢把我给你</p><p>慢慢喜欢你</p><p>慢慢的回忆</p><p>慢慢的陪你慢慢的老去</p><p>因为慢慢是个最好的原因</p><p>晚餐后的甜点就点你喜欢的吧</p><p>今晚就换你去床的右边睡吧</p><p>这次旅行我还想去上次的沙滩</p><p>球鞋手表</p><p>袜子和衬衫都已经烫好</p><p>放行李箱</p><p>早上等着你起床</p><p>慢慢喜欢你</p><p>慢慢的亲密</p><p>慢慢聊自己</p><p>慢慢和你走在一起</p><p>慢慢我想配合你</p><p>慢慢把我给你</p><p>慢慢喜欢你</p><p>慢慢的回忆</p><p>慢慢的陪你慢慢的老去</p><p>因为慢慢是个最好的原因</p><p>书里总爱写到喜出望外的傍晚</p><h1 id="水星记"><a class="markdownIt-Anchor" href="#水星记"></a> 水星记</h1><hr><p><a href="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1557335625683&amp;di=e821cb34a226bbcdc9ad6d230cbb9f99&amp;imgtype=0&amp;src=http%3A%2F%2Fp1.music.126.net%2FEzcH0tkNPaJS1Q_p-oXmTQ%3D%3D%2F109951163655650082.jpg" data-fancybox="group" data-caption class="fancybox"><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1557335625683&amp;di=e821cb34a226bbcdc9ad6d230cbb9f99&amp;imgtype=0&amp;src=http%3A%2F%2Fp1.music.126.net%2FEzcH0tkNPaJS1Q_p-oXmTQ%3D%3D%2F109951163655650082.jpg" alt title></a></p><p>着迷于你眼睛</p><p>银河有迹可循</p><p>穿过时间的缝隙</p><p>它依然真实地</p><p>吸引我轨迹</p><p>这瞬眼的光景</p><p>最亲密的距离</p><p>沿着你皮肤纹理</p><p>走过曲折手臂</p><p>做个梦给你</p><p>做个梦给你</p><p>等到看你银色满际</p><p>等到分不清季节更替</p><p>才敢说沉溺</p><p>还要多远才能进入你的心</p><p>还要多久才能和你接近</p><p>咫尺远近却无法靠近的那个人</p><p>要怎么探寻</p><p>要多么幸运</p><p>才敢让你发觉你并不孤寂</p><p>当我还可以再跟你飞行</p><p>环游是无趣</p><p>至少可以</p><p>陪着你</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;慢慢喜欢你&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#慢慢喜欢你&quot;&gt;&lt;/a&gt; 慢慢喜欢你&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;https://timgsa.baidu.com/timg?i
      
    
    </summary>
    
    
      <category term="文艺一下" scheme="http://MrKolen.github.io/tags/%E6%96%87%E8%89%BA%E4%B8%80%E4%B8%8B/"/>
    
  </entry>
  
  <entry>
    <title>今天是个好日子</title>
    <link href="http://MrKolen.github.io/2019/05/08/%E4%BB%8A%E5%A4%A9%E6%98%AF%E4%B8%AA%E5%A5%BD%E6%97%A5%E5%AD%90/"/>
    <id>http://MrKolen.github.io/2019/05/08/今天是个好日子/</id>
    <published>2019-05-08T09:12:07.000Z</published>
    <updated>2020-02-14T18:15:20.787Z</updated>
    
    <content type="html"><![CDATA[<p>傲娇插旗.jpg</p><a id="more"></a><h1 id="收到一条来自互联网界的广播"><a class="markdownIt-Anchor" href="#收到一条来自互联网界的广播"></a> 收到一条来自互联网界的广播</h1><p>Bibibibibi…(中二线接入中，频段ID：4399—2048)…</p><blockquote><p>2019年5月8日，一名江湖代号为Kolen的菜鸡程序员，历经千辛万苦，不畏千难万阻，终于在辽阔、伟大的互联网世界打下一毫江山，插上了一面伟大的“IT—K”旗帜🚩！！！</p></blockquote><blockquote><p>这是整个互联网界压根算不存在的一步，却是这位小辣鸡🐔在其渺小而又短暂的人生历程上伟大一步！</p></blockquote><p>Emmmm…可恶的营销组织，又来推送垃圾消息</p><p>老板！换碟！</p><h1 id="定个调"><a class="markdownIt-Anchor" href="#定个调"></a> 定个调</h1><p>现在学习都提倡“要有输入输出”的，输入是起点，输出才更是重点，否则同样是一份食材，大佬们做出来的叫“夫妻肺片”，菜鸡做出来的只能叫“凉拌牛杂&quot;。扯道理多没意思，不如直接干来得痛快。于是本着对计算机学科的兴趣和好奇，充分发扬本人的探（杠）索（精）精神，搭建了这个Blog用来记录下未来在人工智能领域学习的过程。</p><blockquote><p>生活不止有眼前的代码，还有诗和远方。      ---- 反正不是鲁迅先生说的</p></blockquote><p>当然，你看我像是个只会一本正经搞技术的人嘛[手动滑稽.jpg]。我打算以后在博客中写些影评、读后感，实用有趣的技术帖、PO些自己的手残摄影，反正就是拒绝无聊，探索生活更多的可能性。欢迎大家来和我探讨哲♂学和艺术~</p><p>第一次写博客，文风和排版都很烂，大佬们路过请轻喷，就这样吧…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;傲娇插旗.jpg&lt;/p&gt;
    
    </summary>
    
      <category term="杂记" scheme="http://MrKolen.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
      <category term="心情" scheme="http://MrKolen.github.io/tags/%E5%BF%83%E6%83%85/"/>
    
      <category term="建博大业" scheme="http://MrKolen.github.io/tags/%E5%BB%BA%E5%8D%9A%E5%A4%A7%E4%B8%9A/"/>
    
  </entry>
  
</feed>
